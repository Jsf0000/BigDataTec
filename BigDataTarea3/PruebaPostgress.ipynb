{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estudiante: Jason Solano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba para probar la conexión descripta por el profesor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probamos la configuración inical para poder realizar la conexión con postgresql, en una base de datos local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, udf \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "dir = \"/Users/jasonsolano/Documents/BigData/BigDataTEC/Clase1/DB/\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Basic JDBC pipeline\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", dir+\"postgresql-42.1.4.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", dir+\"postgresql-42.1.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Reading single DataFrame in Spark by retrieving all rows from a DB table.\n",
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/ShoppingDB\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"102800\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos varios ejercicios descritos en el jupyter notebook suministrado por el profesor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|\n",
      "+---+-----------+------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_df = df.withColumn(\"date_string\", date_format(col(\"purchased_at\"), 'MM/dd/yyyy'))\n",
    "formatted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "| id|customer_id|amount|       purchased_at|date_string|      date|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  2|          1|   125|2017-03-01 10:00:00| 03/01/2017|2017-03-01|\n",
      "|  3|          1|    32|2017-03-02 13:00:00| 03/02/2017|2017-03-02|\n",
      "|  4|          1|    64|2017-03-02 15:00:00| 03/02/2017|2017-03-02|\n",
      "|  5|          1|   128|2017-03-03 10:00:00| 03/03/2017|2017-03-03|\n",
      "|  6|          2|   333|2017-03-01 09:00:00| 03/01/2017|2017-03-01|\n",
      "|  7|          2|   334|2017-03-01 09:01:00| 03/01/2017|2017-03-01|\n",
      "|  8|          2|   333|2017-03-01 09:02:00| 03/01/2017|2017-03-01|\n",
      "|  9|          2|    11|2017-03-03 20:00:00| 03/03/2017|2017-03-03|\n",
      "| 10|          2|    44|2017-03-03 20:15:00| 03/03/2017|2017-03-03|\n",
      "+---+-----------+------+-------------------+-----------+----------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- amount: integer (nullable = true)\n",
      " |-- purchased_at: timestamp (nullable = true)\n",
      " |-- date_string: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_to_date = \\\n",
    "    udf(lambda text_date: datetime.strptime(text_date, '%m/%d/%Y'),\n",
    "        DateType())\n",
    "\n",
    "typed_df = formatted_df.withColumn(\"date\", string_to_date(formatted_df.date_string))\n",
    "typed_df.show()\n",
    "typed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+----------------+-----------+\n",
      "|customer_id|      date|sum(id)|sum(customer_id)|sum(amount)|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "|          2|2017-03-01|     21|               6|       1000|\n",
      "|          1|2017-03-02|      7|               2|         96|\n",
      "|          1|2017-03-03|      5|               1|        128|\n",
      "|          1|2017-03-01|      3|               2|        180|\n",
      "|          2|2017-03-03|     19|               4|         55|\n",
      "+-----------+----------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df = typed_df.groupBy(\"customer_id\", \"date\").sum()\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n",
      "+-----------+----------+------+\n",
      "|customer_id|      date|amount|\n",
      "+-----------+----------+------+\n",
      "|          2|2017-03-01|  1000|\n",
      "|          1|2017-03-02|    96|\n",
      "|          1|2017-03-03|   128|\n",
      "|          1|2017-03-01|   180|\n",
      "|          2|2017-03-03|    55|\n",
      "+-----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats_df = \\\n",
    "    sum_df.select(\n",
    "        col('customer_id'),\n",
    "        col('date'),\n",
    "        col('sum(amount)').alias('amount'))\n",
    "\n",
    "stats_df.printSchema()\n",
    "stats_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+---+----+--------+\n",
      "| id|name|currency|\n",
      "+---+----+--------+\n",
      "|  1|John|     CRC|\n",
      "|  2|Jane|     EUR|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n",
    "\n",
    "names_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"names.csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(StructType([\n",
    "                StructField(\"id\", IntegerType()),\n",
    "                StructField(\"name\", StringType()),\n",
    "                StructField(\"currency\", StringType())])) \\\n",
    "    .load()\n",
    "\n",
    "names_df.printSchema()\n",
    "names_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|customer_id|      date|amount| id|name|currency|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "|          2|2017-03-01|  1000|  2|Jane|     EUR|\n",
      "|          1|2017-03-02|    96|  1|John|     CRC|\n",
      "|          1|2017-03-03|   128|  1|John|     CRC|\n",
      "|          1|2017-03-01|   180|  1|John|     CRC|\n",
      "|          2|2017-03-03|    55|  2|Jane|     EUR|\n",
      "+-----------+----------+------+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joint_df = stats_df.join(names_df, stats_df.customer_id == names_df.id)\n",
    "joint_df.printSchema()\n",
    "joint_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "- Cree un pequeño generador de transacciones. Para ello, puede utilizar funciones de numpy o pandas, para crear las transacciones nuevas en memoria y, posteriormente, puede cargarlas a un *Spark Dataframe* que después debe insertar en la base de datos.\n",
    "- Con los datos nuevos, ejecute el código de éste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos una función para obtener dias random con el inicio de una determinada fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01T01:01:11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def random_date_generator(start_date, range_in_days):\n",
    "    days_to_add = np.arange(1, range_in_days)\n",
    "    random_date = np.datetime64(start_date) + np.random.choice(days_to_add)\n",
    "    return random_date\n",
    "print(random_date_generator('2017-01-01 01:00:00',360))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### se realiza una lista que guarda registros con los atributos requeridos para la tabla transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 788, numpy.datetime64('2017-10-24')), (1, 857, numpy.datetime64('2017-02-02')), (0, 490, numpy.datetime64('2017-11-21')), (1, 39, numpy.datetime64('2017-11-27')), (0, 907, numpy.datetime64('2017-02-28')), (0, 351, numpy.datetime64('2017-06-16')), (1, 158, numpy.datetime64('2017-03-24')), (1, 373, numpy.datetime64('2017-03-04')), (1, 227, numpy.datetime64('2017-06-26'))]\n"
     ]
    }
   ],
   "source": [
    "limite = 10\n",
    "contador = 0\n",
    "listData = list()\n",
    "while(True):\n",
    "    contador+=1\n",
    "    if(contador == limite):\n",
    "        break\n",
    "    else:\n",
    "        randomId = np.random.choice(2)\n",
    "        randomAmount = np.random.choice(1000)\n",
    "        randomDate = random_date_generator('2017-01-01',360)\n",
    "        listData.append((randomId,randomAmount,randomDate))\n",
    "print(listData)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crea un dataframe con los registros con valores random, basados en la tabla transactions. \n",
    "### Para este caso es necesario crear una columna \"date\" con datos en string y una columna \"purchased_at\" con formato timestamp.\n",
    "\n",
    "### Luego los datos de la columna \"date\" son tranformados al formato timestamp en la columna \"purchased_at\", y posterior esta columna es borrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "session=SparkSession.builder.appName('data_processing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+-------------------+\n",
      "|customer_id|amount|       purchased_at|\n",
      "+-----------+------+-------------------+\n",
      "|          0|   788|2017-10-24 00:00:00|\n",
      "|          1|   857|2017-02-02 00:00:00|\n",
      "|          0|   490|2017-11-21 00:00:00|\n",
      "|          1|    39|2017-11-27 00:00:00|\n",
      "|          0|   907|2017-02-28 00:00:00|\n",
      "|          0|   351|2017-06-16 00:00:00|\n",
      "|          1|   158|2017-03-24 00:00:00|\n",
      "|          1|   373|2017-03-04 00:00:00|\n",
      "|          1|   227|2017-06-26 00:00:00|\n",
      "+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "schema = StructType().add(\"customer_id\",\"integer\").add(\"amount\",\"integer\").add(\"date\",'string')\n",
    "dataframeSpark = session.createDataFrame(data,schema=schema)\n",
    "dataframeSpark = dataframeSpark.withColumn('purchased_at', col('date').cast('timestamp'))\n",
    "dataframeSpark = dataframeSpark.drop('date')\n",
    "dataframeSpark.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se guardan los registros anteriormente mencionados en la tabla transactions en el motor prostgresql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeSpark.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost/ShoppingDB',\n",
    "      dbtable='transactions',\n",
    "      user='postgres',\n",
    "      password='102800').mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volvemos a leer los datos desde el motor de la base de datos en postgresql,  en el cual vemos los datos insertados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+-------------------+\n",
      "| id|customer_id|amount|       purchased_at|\n",
      "+---+-----------+------+-------------------+\n",
      "|  1|          1|    55|2017-03-01 09:00:00|\n",
      "|  2|          1|   125|2017-03-01 10:00:00|\n",
      "|  3|          1|    32|2017-03-02 13:00:00|\n",
      "|  4|          1|    64|2017-03-02 15:00:00|\n",
      "|  5|          1|   128|2017-03-03 10:00:00|\n",
      "|  6|          2|   333|2017-03-01 09:00:00|\n",
      "|  7|          2|   334|2017-03-01 09:01:00|\n",
      "|  8|          2|   333|2017-03-01 09:02:00|\n",
      "|  9|          2|    11|2017-03-03 20:00:00|\n",
      "| 10|          2|    44|2017-03-03 20:15:00|\n",
      "| 12|          0|   788|2017-10-24 00:00:00|\n",
      "| 13|          0|   490|2017-11-21 00:00:00|\n",
      "| 11|          1|   158|2017-03-24 00:00:00|\n",
      "| 14|          0|   907|2017-02-28 00:00:00|\n",
      "| 15|          1|   857|2017-02-02 00:00:00|\n",
      "| 16|          0|   351|2017-06-16 00:00:00|\n",
      "| 17|          1|    39|2017-11-27 00:00:00|\n",
      "| 18|          1|   373|2017-03-04 00:00:00|\n",
      "| 19|          1|   227|2017-06-26 00:00:00|\n",
      "+---+-----------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost/ShoppingDB\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"102800\") \\\n",
    "    .option(\"dbtable\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
